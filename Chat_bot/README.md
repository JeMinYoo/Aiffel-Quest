# 회고
- 김주현
    - 케창딥 책에 포함된 트랜스포머 코드와는 또 달리 좀 더 로우 레벨로 작성된 LMS 노드 학습의 코드를 통해 트랜스포머의 구조와 동작 방식에 대해 좀 더 명확히 이해하는 계기가 되었습니다.
    - 사소한 내용이어도 동료 그루분들의 질문과 궁금증이 자극이 되어 좀 더 자세히 알아보고 정확히 이해하기 위해 노력할 수 있어서 많은 도움이 되고 있음.
    - 동일한 코드를 로컬 컴퓨터 또는 코랩에서 돌려보면서 개선하고 싶었으나, 로컬 컴퓨터와 코랩에서는 인코더와 디코더간, 디코더의 예측 시퀀스와 타깃 시퀀스 간의 차원 불일치 등이 계속 발생해 해당 이슈를 해결하기 위해 집중하다 보니 정작 프로젝트에는 많은 시간을 쏟지 못한 점은 아쉬움으로 남음.

- 유제민
    - 처음 챗봇을 만들 때 KoBERT 모델을 사용해 만들려 했으나 KoBERT는 감정 기반 모델이다 보니 일반적인 챗봇과는 맞지 않다 생각하여 GPT2로 변경했습니다.
    - 결과는 그리 좋지 않았으나 모델이 개선되는 것이 눈에 보여 기분은 좋았습니다만 nlp 프로젝트를 진행하면서 점점 더어렵다…라는 생각이 많이 들었고 nlp 공부를 더 해야겠다고 느꼈습니다.
 
- 김원영
    - 챗봇 데이터를 성공적으로 수집하고 전처리하였는데 여기에는 비한국어 문자를 제거하고 누락된 값을 처리하는 작업이 포함되어 있으며 SubwordTextEncoder를 사용하여 토큰화를 구현하고 시작 및 종료 토큰을 추가하며  시퀀스를 최대 길이로 패딩하는 과정을 거쳐 형태소 분석 및 불용어 제거를 위해 KoNLPy를 사용하여 한국어 텍스트 전처리 파이프라인을 강화하여 모델의 입력 데이터 품질을 향상시키려 노력하였습니다.
    - 그런 후에,  TensorFlow를 사용하여 데이터셋을 배치 및 셔플링하여 학습을 준비하였고 Teacher forcing과 적절한 데이터셋 처리를 통해 효율적인 모델 학습이 되도록 실행하였습니다.  마지막으로, 사용자 인터페이스를 위해 사용자가 입력창에 문장을 입력하고 “Send” 버튼을 클릭하면 챗봇이 응답을 생성하여 화면에 표시되도록 Streamlit을 구현하는 단계까지 진행했습니다.
    - 그러나, 구현된 Transformer 모델의 훈련 부족 등의 이유로 구현된 챗봇은 한국어 입력문장에 아주 맥락에 맞는 한국어로 답변을 만드는데까지는 시간이 부족했습니다. 하기처럼, 기계가 단답형의 문답을 내놓은 수준이었습니다.
    - 입력 : 사내커플인데 비밀연애임 답답해
      출력 : 헤어진다
    - 가독성 향상과 디버깅 용이를 위해 코드를 간결하게 만들어야 함을 인지하면서도 불필요한 코드나 중복된 코드를 제거하는데 있어서 여전히 보수적인 성향을 바꾸는데 어려움을 느끼고 있습니다.
