{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMuEytxk/flyTaBboprkxC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeMinYoo/Aiffel-Quest/blob/master/Main_quest/Quest05/DLThon_ToiToy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1 데이터 전처리 및 토크나이징\n"
      ],
      "metadata": {
        "id": "ZSA2cxb_fhZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 한국어 단발성 대화 데이터셋"
      ],
      "metadata": {
        "id": "_HD4C5IrirVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 KoBERT"
      ],
      "metadata": {
        "id": "JdWUbkfvnAy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# [AI Hub] 감정 분류를 위한 대화 음성 데이터셋\n",
        "chatbot_concat_data  = pd.read_excel('/content/gdrive/MyDrive/Colab Notebooks/concat_talks.xlsx')\n",
        "\n",
        "chatbot_concat_data.sample(n=10)\n",
        "# 7개의 감정 class → 숫자\n",
        "chatbot_data.loc[(chatbot_data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
        "chatbot_data.loc[(chatbot_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
        "chatbot_data.loc[(chatbot_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
        "chatbot_data.loc[(chatbot_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
        "chatbot_data.loc[(chatbot_data['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
        "chatbot_data.loc[(chatbot_data['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
        "chatbot_data.loc[(chatbot_data['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6\n",
        "\n",
        "data_list = []\n",
        "for q, label in zip(chatbot_concat_data['Sentence'], chatbot_concat_data['Emotion'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)\n",
        "\n",
        "print(data)\n",
        "print(data_list[:10])"
      ],
      "metadata": {
        "id": "JtmuuqVsnGva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, shuffle = True, random_state = 32)"
      ],
      "metadata": {
        "id": "N3TZNvQNqU0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "UxBMCyn8nUSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTDataset : 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 tokenization, int encoding, padding하는 함수\n",
        "tok = tokenizer.tokenize\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)\n",
        "\n",
        "# torch 형식의 dataset을 만들어 입력 데이터셋의 전처리 마무리\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 5)"
      ],
      "metadata": {
        "id": "xBLk1kUcnb4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 감성 대화 말뭉치"
      ],
      "metadata": {
        "id": "_rFXjak1g0VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2 베이스 모델"
      ],
      "metadata": {
        "id": "XbFhQmXXgOcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 KoBERT"
      ],
      "metadata": {
        "id": "yn1UNu63hbmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "oMpEQQqxxx6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "device = torch.device(\"cuda:0\")\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
      ],
      "metadata": {
        "id": "HVLt5zCuyDSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "W0ZoQwKCyGoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 64\n",
        "batch_size = 128\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  3e-5"
      ],
      "metadata": {
        "id": "o56Sn-CZyGtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTDataset : 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 tokenization, int encoding, padding하는 함수\n",
        "tok = tokenizer.tokenize\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)\n",
        "\n",
        "# torch 형식의 dataset을 만들어 입력 데이터셋의 전처리 마무리\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 5)"
      ],
      "metadata": {
        "id": "sciJs0fOyGxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 7,   # 감정 클래스 수로 조정\n",
        "                 dr_rate = None,\n",
        "                 params = None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p = dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict = False)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "hI9SucP-yJSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT  모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate = 0.7).to(device)\n",
        "\n",
        "# optimizer와 schedule 설정\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}\n",
        "]\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 loss function\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total)\n",
        "\n",
        "# calc_accuracy : 정확도 측정을 위한 함수\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "eE9NkTXYyJWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "smMhVWPFyJc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과\n",
        "train acc는 0.85\n",
        "test acc가 0.55"
      ],
      "metadata": {
        "id": "MYF7DIvmyoHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 KcELECTRA"
      ],
      "metadata": {
        "id": "7sCYXJsuhk10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 KoNLPy"
      ],
      "metadata": {
        "id": "yO576bovegR8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZQYKZCOekxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3 모델 성능 개선"
      ],
      "metadata": {
        "id": "GHMyjwD7cWtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 KoBERT"
      ],
      "metadata": {
        "id": "jVVvOVmOjOHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 사이즈와 학습률, 드롭아웃을 조정해 봤으나 개선되지 않았다.\n",
        "데이터셋의 문제로 판단하여 기존 데이터셋(한국어 단발성 대화)와 감성 대화 말뭉치를 합치고 7개의 감정을 5개의 감정으로 변경하였고 추가로 데이터 증강 진행"
      ],
      "metadata": {
        "id": "yVX_YIcJzCRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# [AI Hub] 감정 분류를 위한 대화 음성 데이터셋\n",
        "chatbot_concat_data  = pd.read_excel('/content/gdrive/MyDrive/Colab Notebooks/concat_talks.xlsx')\n",
        "\n",
        "chatbot_concat_data.sample(n=10)\n",
        "# 5개의 감정 class → 숫자\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"불안\"), 'Emotion'] = 0  #불안 => 0\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"행복\"), 'Emotion'] = 4  #행복 => 4\n",
        "\n",
        "data_list = []\n",
        "for q, label in zip(chatbot_concat_data['Sentence'], chatbot_concat_data['Emotion'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)\n",
        "\n",
        "print(data)\n",
        "print(data_list[:10])"
      ],
      "metadata": {
        "id": "xJkfYrvkzCBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textattack.augmentation import WordNetAugmenter\n",
        "\n",
        "augmenter = WordNetAugmenter()\n",
        "augmented_data_list = []\n",
        "\n",
        "# 원본 데이터 증강\n",
        "for data in data_list:\n",
        "    sentence, label = data\n",
        "    augmented_sentences = augmenter.augment(sentence)\n",
        "    for augmented_sentence in augmented_sentences:\n",
        "        augmented_data_list.append([augmented_sentence, label])\n",
        "\n",
        "# 증강된 데이터를 원본 데이터에 추가\n",
        "data_list += augmented_data_list\n",
        "\n",
        "# 다시 train & test 데이터로 나누기\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=32)"
      ],
      "metadata": {
        "id": "OR_TaZ9yznax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 5,   # 감정 클래스 수로 조정\n",
        "                 dr_rate = None,\n",
        "                 params = None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p = dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict = False)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "GybR0Y9rzo8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT  모델 불러오기\n",
        "model2 = BERTClassifier(bertmodel,  dr_rate = 0.7).to(device)\n",
        "\n",
        "# optimizer와 schedule 설정\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model2.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model2.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}\n",
        "]\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 loss function\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total)\n",
        "\n",
        "# calc_accuracy : 정확도 측정을 위한 함수\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "U5rkb_yCzpCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model2.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model2(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model2.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model2(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "176u7DUEzsLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과\n",
        "train acc는 0.82\n",
        "test acc가 0.74로 상승"
      ],
      "metadata": {
        "id": "xO7d6rHmzwZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# [AI Hub] 감정 분류를 위한 대화 음성 데이터셋\n",
        "chatbot_concat_data  = pd.read_excel('/content/gdrive/MyDrive/Colab Notebooks/concat_talks.xlsx')\n",
        "\n",
        "chatbot_concat_data.sample(n=10)\n",
        "# 5개의 감정 class → 숫자\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"불안\"), 'Emotion'] = 0  #불안 => 0\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
        "chatbot_concat_data.loc[(chatbot_concat_data['Emotion'] == \"행복\"), 'Emotion'] = 4  #행복 => 4\n",
        "\n",
        "data_list = []\n",
        "for q, label in zip(chatbot_concat_data['Sentence'], chatbot_concat_data['Emotion'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)\n",
        "\n",
        "print(data)\n",
        "print(data_list[:10])"
      ],
      "metadata": {
        "id": "j-VJvPAYbz_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textattack.augmentation import WordNetAugmenter\n",
        "\n",
        "augmenter = WordNetAugmenter()\n",
        "augmented_data_list = []\n",
        "\n",
        "# 원본 데이터 증강\n",
        "for data in data_list:\n",
        "    sentence, label = data\n",
        "    augmented_sentences = augmenter.augment(sentence)\n",
        "    for augmented_sentence in augmented_sentences:\n",
        "        augmented_data_list.append([augmented_sentence, label])\n",
        "\n",
        "# 증강된 데이터를 원본 데이터에 추가\n",
        "data_list += augmented_data_list\n",
        "\n",
        "# 다시 train & test 데이터로 나누기\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=32)"
      ],
      "metadata": {
        "id": "vR0JOXmtb0yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 5,   # 감정 클래스 수로 조정\n",
        "                 dr_rate = None,\n",
        "                 params = None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p = dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict = False)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "7RecheYLb01q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT  모델 불러오기\n",
        "model2 = BERTClassifier(bertmodel,  dr_rate = 0.7).to(device)\n",
        "\n",
        "# optimizer와 schedule 설정\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model2.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model2.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}\n",
        "]\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 loss function\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total)\n",
        "\n",
        "# calc_accuracy : 정확도 측정을 위한 함수\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "auf-r5gWb04p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model2.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model2(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model2.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model2(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "n2e6_6Amb7Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과\n",
        "\n",
        "train acc는 0.86\n",
        "test acc가 0.80로 상승"
      ],
      "metadata": {
        "id": "JIBa3lqsb_lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 KcELECTRA"
      ],
      "metadata": {
        "id": "BGx6pmYijOZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 KoNLPy"
      ],
      "metadata": {
        "id": "-WYT6WnfjO69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4 최종 모델"
      ],
      "metadata": {
        "id": "aLtPvndIjcFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 KcELECTRA"
      ],
      "metadata": {
        "id": "7nRCdwGR4Gp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5 FastAPI"
      ],
      "metadata": {
        "id": "s90HD0Jbc2Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "from fastapi import FastAPI, Request, HTTPException, File, UploadFile, Query\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.middleware.base import BaseHTTPMiddleware\n",
        "from PIL import Image\n",
        "import io\n",
        "import logging\n",
        "import requests\n",
        "from openai import OpenAI   # GPT-4o mini 사용 위해 추가\n",
        "from pathlib import Path    # TTS-1 오디오 파일 생성 경로 설정을 위해 추가\n",
        "from pydantic import BaseModel  # Request 데이터의 유효성 검사 및 문서 자동화를 위해 추가\n",
        "from typing import Optional     # Python의 타입 힌팅을 위해 추가. (Optional을 사용하면 값이 있을 수도 있고 없을 수도 있음을 의미. 즉, 필수가 아닌 선택을 의미함)\n",
        "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification   # KcELECTRA 모델 사용 위해 추가\n",
        "import tensorflow as tf\n",
        "import xmltodict    # XML 파싱을 위해 추가(maniadb.com API)\n",
        "from urllib.parse import quote, urlencode, urlunparse   # URL 인코딩(특히, 한글)을 위해 추가\n",
        "import random\n",
        "\n",
        "\n",
        "# FastAPI 앱 생성\n",
        "app = FastAPI()\n",
        "\n",
        "# CORS configuration\n",
        "origins = [\"*\"]\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 저장해 둔 KcELECTRA 모델과 토크나이저 로드\n",
        "num_labels = 5  # 분석할 감정 레이블 갯수\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"./kcelectra_fine_tuned/tokenizer/\")\n",
        "    electra_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        './kcelectra_fine_tuned/',\n",
        "        num_labels=num_labels)\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to load model: %s\", e)\n",
        "    raise\n",
        "\n",
        "# A simple example of a GET request\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    logger.info(\"Root URL was requested\")\n",
        "    return \"안녕하세요. 저희는 아이펠 DLthon에 참여중인 팀 ToiToy예요. 만나서 반갑습니다.\"\n",
        "\n",
        "\n",
        "# POST 요청 데이터의 유효성 검사와 문서 자동화를 위해 Pydantic 모델 상속받아 커스텀 클래스 정의\n",
        "# 즉, POST 요청을 통해 서버로 전송될 데이터의 형식이라고 보면 됨.\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# [테스트] 문자열을 전달받아 간단한 처리 후 JSON 형식으로 응답\n",
        "@app.post(\"/dlthon_test/\")\n",
        "async def process_text(request: TextRequest):\n",
        "    text = request.text\n",
        "\n",
        "    # 문자열 처리 로직 (예: 문자열을 대문자로 변환)\n",
        "    processed_text = text.upper()\n",
        "\n",
        "    # JSON 응답 생성\n",
        "    response = {\n",
        "        \"original_text\": text,\n",
        "        \"processed_text\": processed_text\n",
        "    }\n",
        "\n",
        "    # [참고] FastAPI는 응답 대상이 pydantic 객체, dict, list 타입인 경우에는 JSON 직렬화를 자동으로 함. (예: 아래 return response 처럼)\n",
        "    # (물론, Content-Type 헤더도 'application/json'으로, status code도 '200 OK'로 자동 설정됨)\n",
        "    # 반면, FastAPI의 공식 응답 객체인 JSONResponse 클래스를 사용하면 응답 본문, 상태 코드, 헤더 등을 명시적으로 지정할 수 있음.\n",
        "    # return response\n",
        "    return JSONResponse(content=response)\n",
        "\n",
        "\n",
        "# 문자열을 전달받아 AI 모델에 의해 감정 분석 후 분석 결과를 리턴\n",
        "@app.post(\"/sentiment/\")\n",
        "async def analyze_sentiment(request: TextRequest):\n",
        "    text = request.text     # POST 요청으로 전달된 문자열\n",
        "\n",
        "    # 감정 분석\n",
        "    inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "    logits = electra_model(**inputs).logits\n",
        "    predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])    # [0]은 배치 차원이 있는 경우 첫번째 샘플을 의미함.\n",
        "    predicted_label = electra_model.config.id2label[predicted_class_id]\n",
        "    # Softmax 변환\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    probabilities_list = probabilities.numpy()[0].tolist()\n",
        "    predicted_output = probabilities_list\n",
        "\n",
        "    # # 테스트용\n",
        "    # predicted_output = \"\"\n",
        "    # predicted_label = \"슬픔\"\n",
        "\n",
        "    # ChatGPT-4o-mini에게 요청\n",
        "    try:\n",
        "        MODEL=\"gpt-4o-mini\"     # GPT 모델 지정\n",
        "        client = OpenAI()\n",
        "\n",
        "        # 사용자가 POST 요청으로 전달한 문장 자체를 기준으로 프롬프트 작성\n",
        "        if predicted_label == '행복':\n",
        "            system_content = f\"너는 심리적 어려움을 가지고 있는 사람들을 돕는 전문 상담가야. 내가 입력하는 문장에 대한 공감과 응원의 말을 작성해 줘. 전문 상담가의 느낌이 나도록 가볍지 않은 어투로 작성해 주고 늘 존대말로 답변해 줘. 맨 처음 '안녕하세요', '안녕' 같은 인사말은 하지 않아도 되.\"\n",
        "        else:\n",
        "            system_content = f\"너는 심리적 어려움을 가지고 있는 사람들을 돕는 전문 상담가야. 내가 입력하는 문장에 대한 공감과 위로, 응원의 말을 작성해 줘. 전문 상담가의 느낌이 나도록 가볍지 않은 어투로 작성해 주고 늘 존대말로 답변해 줘. 맨 처음 '안녕하세요', '안녕' 같은 인사말은 하지 않아도 되.\"\n",
        "\n",
        "        # AI 모델이 분석한 결과 레이블 값을 기준으로 프롬프트 작성\n",
        "        # --> [결정적 단점] AI 모델이 분석한 결과 레이블 값이 이상하면 ChatGPT의 답변 역시 이상한 결과가 나옴.\n",
        "        # if predicted_label == '행복':\n",
        "        #     content = f\"{predicted_label}의 감정을 가지고 있는 사람에게 도움이 되도록 공감과 응원의 말을 작성해 줘. 전문 상담가의 느낌이 나도록 가볍지 않은 어투로 작성해 주고 늘 존대말로 답변해 줘. 맨 처음 '안녕하세요', '안녕' 같은 인사말은 하지 않아도 되.\"\n",
        "        # else:\n",
        "        #     content = f\"{predicted_label}의 감정을 가지고 있는 사람에게 도움이 되도록 공감과 위로, 응원의 말을 작성해 줘. 전문 상담가의 느낌이 나도록 가볍지 않은 어투로 작성해 주고 늘 존대말로 답변해 줘. 맨 처음 '안녕하세요', '안녕' 같은 인사말은 하지 않아도 되.\"\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_content},\n",
        "                # {\"role\": \"user\", \"content\": content}      # AI 모델이 분석한 결과 레이블 기준으로 요청하는 경우\n",
        "                {\"role\": \"user\", \"content\": text}       # 사용자가 POST 요청으로 전달한 문장을 그대로 사용해서 요청하는 경우\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        answer = completion.choices[0].message.content\n",
        "\n",
        "        if predicted_label == '행복':\n",
        "            answer += \"\\n\\n앞으로도 더욱 멋진 날들이 이어지기를 바라는 마음으로 노래를 몇 곡 준비해 봤습니다. 멋진 노래와 함께 하루를 잘 마무리하길 바랄게요!\"\n",
        "        else:\n",
        "            answer += \"\\n\\n조금이나마 도움이 되길 바라는 마음으로 노래를 몇 곡 준비해 봤습니다. 때로는 몇 마디 말보다 노래 하나가 더 힘이 될 때도 많으니까요.\"\n",
        "    except Exception as e:\n",
        "        logger.error(\"Prediction failed: %s\", e)\n",
        "        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n",
        "\n",
        "    # JSON 응답 생성\n",
        "    response = {\n",
        "        \"probabilities\": predicted_output,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "    return JSONResponse(content=response)\n",
        "\n",
        "\n",
        "# 감정에 해당하는 단어를 전달받아 공감과 위로, 응원이 되는 문장을 생성해서 리턴\n",
        "@app.get('/comfort/')\n",
        "async def generate_words_of_comfort(sentiment: str = Query(...)):\n",
        "    # ChatGPT-4o-mini에게 요청\n",
        "    try:\n",
        "        MODEL=\"gpt-4o-mini\"     # GPT 모델 지정\n",
        "        client = OpenAI()\n",
        "\n",
        "        if sentiment == '행복':\n",
        "            content = f\"{sentiment}의 감정을 가지고 있는 사람에게 도움이 되도록 공감과 응원의 말을 작성해 줘. 친근감이 느껴지도록 친구와 대화하는 느낌의 반말로 작성해 줘. 맨 처음 '안녕하세요', '안녕' 같은 인사말은 하지 않아도 되.\"\n",
        "        else:\n",
        "            content = f\"{sentiment}의 감정을 가지고 있는 사람에게 도움이 되도록 공감과 위로, 응원의 말을 작성해 줘. 친근감이 느껴지도록 친구와 대화하는 느낌의 반말로 작성해 줘. 맨 처음 '안녕하세요', '안녕' 같은 인사말은 하지 않아도 되.\"\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"너는 심리적 어려움을 가지고 있는 사람들을 돕는 전문 상담가야.\"},\n",
        "                {\"role\": \"user\", \"content\": content}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        answer = completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        logger.error(\"Prediction failed: %s\", e)\n",
        "        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n",
        "\n",
        "    # JSON 응답 생성\n",
        "    response = {\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "    return JSONResponse(content=response)\n",
        "\n",
        "\n",
        "# 감정에 해당하는 단어를 전달받아 그에 해당하는 음악 리스트를 리턴 (maniadb.com API 활용)\n",
        "# mainadb.com API에서는 응답을 XML 형식으로 전달하기 때문에 XML을 파싱해서 필요한 엘리먼트만 추출해 JSON 형식으로 변환해야 함!\n",
        "@app.get('/music/')\n",
        "async def generate_music_list(sentiment: str = Query(...)):\n",
        "    # 한글 단어와 쿼리 파라미터\n",
        "    search_term = sentiment  # 감정 단어('공포', '분노', '불안', '슬픔', '행복')\n",
        "    base_url = \"https://www.maniadb.com/api/search/\"\n",
        "    query_params = {\n",
        "        \"sr\": \"song\",\n",
        "        \"display\": 5,\n",
        "        \"key\": \"example\",\n",
        "        \"v\": \"0.5\"\n",
        "    }\n",
        "\n",
        "    # 한글 단어를 URL 인코딩\n",
        "    encoded_search_term = quote(search_term)\n",
        "    # 쿼리 파라미터를 URL 인코딩\n",
        "    encoded_query_params = urlencode(query_params)\n",
        "\n",
        "    # 전체 URL 구성(인코딩 처리된 최종 URL)\n",
        "    url = f\"{base_url}{encoded_search_term}/?{encoded_query_params}\"\n",
        "\n",
        "    try:\n",
        "        # XML 데이터 요청\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # HTTP 오류 발생 시 예외 발생\n",
        "\n",
        "        # XML 데이터를 JSON으로 변환\n",
        "        xml_data = response.text\n",
        "        json_data = xmltodict.parse(xml_data)\n",
        "\n",
        "        # 'item' 태그만 추출\n",
        "        items = json_data['rss']['channel']['item']\n",
        "\n",
        "        # item에서 필요한 데이터만 추출\n",
        "        results = []\n",
        "\n",
        "        for item in items:\n",
        "            result = {\n",
        "                'title': item.get('title', ''),\n",
        "                'artist': item.get('maniadb:artist', {}).get('name', ''),  # name 태그는 maniadb:artist 안에 있음\n",
        "                'link': item.get('link', ''),\n",
        "                'album_title': item.get('maniadb:album', {}).get('title', ''),  # title 태그는 maniadb:album 안에 있음\n",
        "                'album_link': item.get('maniadb:album', {}).get('link', ''),  # link 태그는 maniadb:album 안에 있음\n",
        "                'album_image': item.get('maniadb:album', {}).get('image', '')  # image 태그는 maniadb:album 안에 있음\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        # JSON 응답 반환\n",
        "        return JSONResponse(content={\"items\": results})\n",
        "    # except requests.exceptions.RequestException as e:\n",
        "    #     return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "    except Exception as e:\n",
        "        logger.error(\"maniadb.com API call failed: %s\", e)\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Internal Server Error\")\n",
        "\n",
        "\n",
        "# 감정에 해당하는 단어를 전달받아 그에 해당하는 음악 리스트를 리턴 (maniadb.com API 활용)\n",
        "# mainadb.com API에서는 응답을 XML 형식으로 전달하기 때문에 XML을 파싱해서 필요한 엘리먼트만 추출해 JSON 형식으로 변환해야 함!\n",
        "# [중요] /music/과의 차이점 : /music2/는 감정에 해당하는 단어와 연관된 단어 중에서 랜덤하게 선택해 API 호출하는 방식.\n",
        "# API 호출이 5번 이루어지기 때문에 응답 시간이 좀 더 오래 걸림! (평균 10초 내외. /music/은 3초)\n",
        "@app.get('/music2/')\n",
        "async def generate_music_list2(sentiment: str = Query(...)):\n",
        "    # 한글 단어와 쿼리 파라미터\n",
        "    search_term = sentiment  # 감정 단어('공포', '분노', '불안', '슬픔', '행복')\n",
        "    base_url = \"https://www.maniadb.com/api/search/\"\n",
        "    query_params = {\n",
        "        \"sr\": \"song\",\n",
        "        \"display\": 1,   # API 호출당 검색 노래 수(호출할 때마다 키워드로 사용되는 단어가 달라져서 일부러 1로 설정함)\n",
        "        \"key\": \"example\",\n",
        "        \"v\": \"0.5\"\n",
        "    }\n",
        "\n",
        "    # 감정별 연관 단어 리스트를 포함하는 딕셔너리\n",
        "    emotion_words = {\n",
        "        '공포': [\n",
        "            '용기', '안전', '평화', '극복', '보호', '신뢰', '빛', '온기', '포옹', '조용', '고요', '평온', '명상', '호흡', '기도'\n",
        "        ],\n",
        "        '분노': [\n",
        "            '용서', '평정', '이해', '생각', '차분', '조절', '수용', '인내', '연민', '관용', '화해', '평화', '해방', '안도', '명상', '호흡', '자연', '물', '바람', '흙', '공기'\n",
        "        ],\n",
        "        '불안': [\n",
        "            '안정', '평온', '편안', '차분', '긍정', '희망', '용기', '믿음', '조절', '호흡', '명상', '자연', '온기', '포옹', '안전', '보호', '휴식'\n",
        "        ],\n",
        "        '슬픔': [\n",
        "            '위로', '공감', '위안', '치유', '회복', '희망', '긍정', '온기', '포옹', '사랑', '기억', '추억', '시간', '변화', '용기', '미소', '행복'\n",
        "        ],\n",
        "        '행복': [\n",
        "            '행복', '기쁨', '감사', '만족', '사랑', '평화', '행운', '즐거움', '환희', '축복', '긍정', '희망', '열정', '활력', '성장', '발전', '성공', '자유', '만족'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # GET 요청시 넘어온 search_term 감정과 연관된 단어 중에서 랜덤하게 5개만 뽑기\n",
        "    search_random_words = random.sample(emotion_words[search_term], 5)\n",
        "    print(search_random_words)\n",
        "\n",
        "    # 한글 단어 URL 인코딩 : 랜덤하게 추출된 5개 단어 각각에 대해 처리해서 리스트로 저장\n",
        "    encoded_search_term_list = [quote(word) for word in search_random_words]\n",
        "\n",
        "    # 쿼리 파라미터를 URL 인코딩 : 공통이라 한 번만 하면 됨.\n",
        "    encoded_query_params = urlencode(query_params)\n",
        "\n",
        "    # 인코딩된 한글 단어 리스트를 순회하면서 최종 url 리스트 생성\n",
        "    url_list = []\n",
        "    for encoded_search_term in encoded_search_term_list:\n",
        "        url_list.append(f\"{base_url}{encoded_search_term}/?{encoded_query_params}\")\n",
        "\n",
        "    try:\n",
        "        # 5개의 음악 리스트를 저장\n",
        "        results = []\n",
        "\n",
        "        # 5개의 url 리스트 각각을 순회하면서 처리 --> 총 5회의 API 호출 발생함.\n",
        "        for url in url_list:\n",
        "            # XML 데이터 요청\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # HTTP 오류 발생 시 예외 발생\n",
        "\n",
        "            # XML 데이터를 JSON으로 변환\n",
        "            xml_data = response.text\n",
        "            json_data = xmltodict.parse(xml_data)\n",
        "\n",
        "            # 'item' 태그만 추출\n",
        "            # [중요] 음악 하나만 가져오는 상황이라 [0] 없이 그냥 ['item']으로 끝나도 됨. (print문으로 직접 확인함)\n",
        "            item = json_data['rss']['channel']['item']\n",
        "\n",
        "            result = {\n",
        "                'title': item.get('title', ''),\n",
        "                'artist': item.get('maniadb:artist', {}).get('name', ''),  # name 태그는 maniadb:artist 안에 있음\n",
        "                'link': item.get('link', ''),\n",
        "                'album_title': item.get('maniadb:album', {}).get('title', ''),  # title 태그는 maniadb:album 안에 있음\n",
        "                'album_link': item.get('maniadb:album', {}).get('link', ''),  # link 태그는 maniadb:album 안에 있음\n",
        "                'album_image': item.get('maniadb:album', {}).get('image', '')  # image 태그는 maniadb:album 안에 있음\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        # JSON 응답 반환\n",
        "        return JSONResponse(content={\"items\": results})\n",
        "    # except requests.exceptions.RequestException as e:\n",
        "    #     return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "    except Exception as e:\n",
        "        logger.error(\"maniadb.com API call failed: %s\", e)\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Internal Server Error\")\n",
        "\n",
        "\n",
        "# 앱 실행\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"server_fastapi_dlthon:app\",\n",
        "            reload= True,   # Reload the server when code changes\n",
        "            host=\"127.0.0.1\",   # Listen on localhost\n",
        "            port=5000,   # Listen on port 5000\n",
        "            log_level=\"info\"   # Log level\n",
        "            )"
      ],
      "metadata": {
        "id": "HNo6twBFevpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}